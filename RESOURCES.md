# Resources

This section provides links to a subset of the resources linked from the book and some additional related 
resources, especially with code or videos.

## Table of Contents
1. [Online Resources for Getting Started With Deep Learning](#Online-Resources-for-Getting-Started-With-Deep-Learning)
2. [Collaborative Projects](#Collaborative-Projects)
3. [Adversarial Examples Code and Experimentation](#Adversarial-Examples-Code-and-Experimentation)
4. [Fooling Humans](#Fooling-Humans)

---
## Online Resources for Getting Started With Deep Learning
Here are some nice resources. There are many more available online.

### 3Blue1Brown

Four superb introductory videos explaining the mathematics underpinning 
neural networks are here: 
[3Blue1Brown Deep Learning](https://www.youtube.com/watch?v=aircAruvnKk&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi)

### Keras and Tensorflow
To get started with Keras and Tensorflow, the online documentation provides excellent tutorials
[Keras docs](https://keras.io/#you-have-just-found-keras)


### Andrew Ng’s Coursera course

For a complete introduction to all things ML, this is a fantastic course [https://www.coursera.org/learn/machine-learning]().

---
## Collaborative Projects

### Cleverhans 
An open source library for the development of attacks and associated defenses 
with the aim of benchmarking Machine Learning systems’ vulnerability to adversarial examples.
The code repository for Cleverhans is at [https://github.com/openai/cleverhans]()

### Foolbox
A toolbox for creating adversarial examples to enable testing of defenses.
The documentation for Foolbox is at [https://fool box.readthedocs.io/en/latest/]().

### IBM’s Adversarial Robustness Toolbox
This library includes adversarial attacks, defenses, and detection. 
It also supports robustness metrics measurements. 
The code repository for this library is here: [https://github.com/IBM/ adversarial-robustness-toolbox]().

### RobustML 
Robust ML aims to provide a central website for learning about defenses and their analyses and evaluations. 
It is located at [https://www.robust-ml.org/]().

### Competitions
Several competitions have encouraged participation in the generation of adversarial 
attacks and defenses including competitions from Google 
and Kaggle ([https://www.kaggle.com]()).

---
## Early Papers for Concepts of Adversarial Examples

* [Intriguing properties of neural networks by 
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, Rob Fergus](https://arxiv.org/abs/1312.6199)

* [Deep Neural Networks are Easily Fooled: High Confidence Predictions for Unrecognizable Images
by Anh Nguyen, Jason Yosinski, Jeff Clune](https://arxiv.org/abs/1412.1897)

---
## Adversarial Examples Code and Experimentation

Many research papers have associated GitHub repositories and videos/audio examples. Here are a few.

### Adversarial Patch

To see adversarial patches in action, take a look at this video:
[Adversarial Patch on YouTube](https://www.youtube.com/watch?v=i1sp4X57TL4&feature=youtu.be).
This accompanies the paper [Adversarial Patch by Brown et al.](https://arxiv.org/abs/1712.09665).
Example code for creating adversarial patches is [here](https://github.com/tensorflow/cleverhans/tree/master/examples/adversarial_patch)

### Creating Robust and Physical World Examples

Here's a well presented [Jupyter notebook](https://www.anishathalye.com/2017/07/25/synthesizing-adversarial-examples/) to accompany the paper 
[Synthesizing Robust Adversarial Examples by Anish Athalye, Logan Engstrom, Andrew Ilyas, and Kevin Kwok](https://arxiv.org/abs/1707.07397).

This research considers a different approach, raising the possibility of fooling neural networks by viewing objects from unusual angles.
[Strike (with) a Pose: Neural networks are easily fooled by strange poses of familiar objects 
by Michael Alcorn, Qi Li, Zhitao Gong, Chengfei Wang, Long Mai, Wei-shinn Ku, Anh Nguyen](http://anhnguyen.me/project/strike-with-a-pose/)

### Adversarial Audio 

If you'd like to listen to some adversarial audio examples, take a look [here](https://nicholas.carlini.com/code/audio_adversarial_examples/).
This accompanies the paper [Audi Adversarial Examples Targeted Attacks on Speech-to-Text
by Carlini and Wagner](https://nicholas.carlini.com/papers/2018_dls_audioadvex.pdf)

## Fooling Humans

Humans perception can be tricked in many different ways. Here are some fun examples:

*  [Your brain hallucinates your conscious reality (A. Seth)](https://www.ted.com/talks/anil_seth_how_your_brain_hallucinates_your_conscious_reality)
is an interesting Ted Talk examining how much os what we perceive comes from within.

*  [Everything you hear on a film is a lie (T. Frantzolaz)](https://www.ted.com/talks/tasos_frantzolas_everything_you_hear_on_film_is_a_lie) explains
how we combine multi-sensory input to understand the world.

*  [BBC Two, Try The McGurk Effect! - Horizon: Is Seeing Believing?](https://www.youtube.com/watch?v=G-lN8vWm3m0) shows how 
you can be fooled by conflicting audio and optical input - even when you know you are being tricked. 

*  [Optical Illusions for Kids](https://www.optics4kids.org/illusions) for some basic optical illusions.
